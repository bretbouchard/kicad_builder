"""Lookup part numbers for BOM entries using Mouser and export BOM to JSON.

Behavior:
- Reads `tools/output/bom.csv` (generated by `generate_bom.py`).
- For entries missing `part_number`, queries Mouser Search API with a keyword
  composed from component and value.
- Caches lookup results under `tools/.cache/parts.json` to avoid repeated API calls.
- Writes enriched `tools/output/bom.json` and rewrites `tools/output/bom.csv` with
  populated `part_number` and `manufacturer` where available.

Notes:
- This script reads the `mouser_api_key` environment variable or a `.env` file
  in the repository root. It will not commit or print the API key.
"""

from __future__ import annotations

import csv
import json
import os
import pathlib
import sys
import time
from typing import Dict, Any, Optional, Iterable, List, Tuple, TYPE_CHECKING

try:
    # prefer requests if available at runtime
    import requests  # type: ignore
except Exception:
    requests = None  # type: ignore

if TYPE_CHECKING:
    # Provide a lightweight Any-typed alias for static checkers when
    # types-requests is not installed in the environment.
    from typing import Any as _Any

    requests: _Any  # type: ignore


def load_env(repo_root: pathlib.Path) -> Dict[str, str]:
    # try .env first
    env = {}
    env_path = repo_root / ".env"
    if env_path.exists():
        for ln in env_path.read_text(encoding="utf8").splitlines():
            ln = ln.strip()
            if not ln or ln.startswith("#"):
                continue
            if "=" in ln:
                k, v = ln.split("=", 1)
                env[k.strip()] = v.strip()
    # overlay with real environment variables
    for k, v in os.environ.items():
        env.setdefault(k, v)
    return env


def mouser_search(api_key: str, query: str) -> Optional[Dict[str, Any]]:
    """Do a single Mouser keyword search and return parsed JSON or None."""
    url = "https://api.mouser.com/api/v1/search/keyword"
    params = {"apiKey": str(api_key), "keyword": str(query), "records": str(25)}
    try:
        if requests:
            r = requests.get(url, params=params, timeout=15)
            r.raise_for_status()
            return r.json()
        # urllib fallback (rare)
        from urllib import request, parse

        q = url + "?" + parse.urlencode(params)
        with request.urlopen(q, timeout=15) as fh:
            return json.load(fh)
    except Exception:
        return None


def query_with_retries(
    api_key: str,
    queries: Iterable[str],
    *,
    max_retries: int = 3,
    backoff_base: float = 0.5,
    max_backoff: float = 10.0,
    rate_limit_delay: float = 0.2,
) -> Optional[Dict[str, Any]]:
    """Try a sequence of query strings with retries, backoff, and rate-limiting.

    Returns the first non-empty Mouser response dict, or None.
    """
    for q in queries:
        attempt = 0
        while attempt <= max_retries:
            if attempt > 0:
                # exponential backoff with jitter
                sleep = min(max_backoff, backoff_base * (2 ** (attempt - 1)))
                sleep = sleep * (0.8 + 0.4 * (os.urandom(1)[0] / 255.0))
                time.sleep(sleep)
            # perform request
            resp = mouser_search(api_key, q)
            # polite delay between requests
            time.sleep(rate_limit_delay)
            if resp:
                # prefer responses that actually contain products
                products = resp.get("SearchResults", {}).get("Products") or resp.get("Products")
                if products:
                    return resp
            attempt += 1
    return None


def main():
    repo_root = pathlib.Path(__file__).resolve().parents[2]
    out_dir = repo_root / "tools" / "output"
    out_dir.mkdir(parents=True, exist_ok=True)
    bom_csv = out_dir / "bom.csv"
    bom_json = out_dir / "bom.json"
    cache_dir = repo_root / "tools" / ".cache"
    cache_dir.mkdir(parents=True, exist_ok=True)
    cache_file = cache_dir / "parts.json"

    if not bom_csv.exists():
        print("No BOM found. Run generate_bom.py first.", file=sys.stderr)
        raise SystemExit(1)

    env = load_env(repo_root)
    api_key = env.get("mouser_api_key") or env.get("MOU SER_API_KEY") or env.get("MOU SER_APIKEY")
    # some users store the key as MOU SER_API_KEY â€” try common variants
    api_key = api_key or env.get("MOU SER_API_KEY")

    cache: Dict[str, Any] = {}
    if cache_file.exists():
        try:
            cache = json.loads(cache_file.read_text(encoding="utf8"))
        except Exception:
            cache = {}

    rows: List[Dict[str, str]] = []
    with bom_csv.open("r", encoding="utf8") as f:
        reader = csv.DictReader(f)
        for r in reader:
            rows.append(dict(r))

    for r in rows:
        if r.get("part_number"):
            continue

        comp = (r.get("component") or "").strip()
        val = (r.get("value") or "").strip()
        footprint = (r.get("footprint") or "").strip()

        # Build several query attempts ordered from specific to generic
        queries: List[str] = []
        if comp and val and footprint:
            queries.append(f"{comp} {val} {footprint}")
        if comp and val:
            queries.append(f"{comp} {val}")
        if comp and footprint:
            queries.append(f"{comp} {footprint}")
        if comp:
            queries.append(comp)
        if val:
            queries.append(val)

        # normalize flags for common value synonyms
        def norms(v: str) -> List[str]:
            v = v.replace("uF", "uF").replace("uf", "uF").replace("nF", "nF")
            out = [v]
            if v.endswith("uF") and v.startswith("0."):
                # 0.1uF -> 100nF
                try:
                    num = float(v.replace("uF", ""))
                    if num < 1:
                        out.append(f"{int(num*1000)}nF")
                except Exception:
                    pass
            return out

        norm_vals = norms(val)
        for nv in norm_vals:
            if comp and nv:
                q = f"{comp} {nv}"
                if q not in queries:
                    queries.append(q)

        # skip if nothing to query
        if not queries:
            continue

        # try cache first using primary key (component+value)
        cache_key = f"{comp}|{val}|{footprint}"
        if cache_key in cache:
            hit = cache[cache_key]
            r.setdefault("part_number", hit.get("part_number", ""))
            r.setdefault("manufacturer", hit.get("manufacturer", ""))
            continue

        if not api_key:
            # no key: skip network lookup
            continue

        resp = query_with_retries(api_key, queries)
        if not resp:
            cache[cache_key] = {}
            continue

        products = resp.get("SearchResults", {}).get("Products") or resp.get("Products")
        if not products:
            cache[cache_key] = {}
            continue

        # choose best product by simple scoring: prefer exact value in description
        best: Optional[Tuple[int, Dict[str, Any]]] = None
        for p in products:
            score = 0
            desc = (p.get("Description") or "").upper()
            mf = (p.get("Manufacturer") or "").upper()
            if val and val.upper() in desc:
                score += 3
            if comp and comp.upper() in desc:
                score += 2
            if footprint and footprint.upper() in desc:
                score += 1
            if best is None or score > best[0]:
                best = (score, p)

        p0 = best[1] if best else products[0]
        pn = p0.get("ManufacturerPartNumber") or p0.get("PartNumber") or ""
        mf = p0.get("Manufacturer") or p0.get("Brand") or ""
        r.setdefault("part_number", pn)
        r.setdefault("manufacturer", mf)
        cache[cache_key] = {"part_number": pn, "manufacturer": mf}

    # write enriched JSON and CSV
    bom_json.write_text(json.dumps(rows, indent=2), encoding="utf8")

    fieldnames = (
        rows[0].keys()
        if rows
        else [
            "refdes",
            "component",
            "footprint",
            "package",
            "qty",
            "value",
            "part_number",
            "manufacturer",
            "pins_used",
            "mapping_file",
        ]
    )
    with bom_csv.open("w", newline="", encoding="utf8") as f:
        writer = csv.DictWriter(f, fieldnames=list(fieldnames))
        writer.writeheader()
        for r in rows:
            writer.writerow(r)

    cache_file.write_text(json.dumps(cache, indent=2), encoding="utf8")
    print(f"Wrote enriched BOM to {bom_json} and updated {bom_csv}")


if __name__ == "__main__":
    raise SystemExit(main())
